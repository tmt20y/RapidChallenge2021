# 深層学習Day２
## 勾配消失問題
### ・講義のまとめ
中間層を増やしたときにおこる問題の１つ．誤差逆伝搬法が上手くいかなくなる現象．  
逆伝搬が伝搬されるにしたがって勾配がどんどん緩やかになっていく．そのため伝搬されるべき情報量が少なくなっていき、重みが更新されなくなってしまうことがある．  
これを勾配消失問題という． 

勾配消失を起こす代表的な関数にシグモイド関数がある．  
これはシグモイド関数の微分の最高値が0.25であるためである．  

勾配消失の解決方法には以下の3つがある．  
1．活性化関数の選択  
ReLu関数  
現在最も使われている活性化関数．0より小さい時は微分値が0，0より大きい時は微分値が1になる．  
この微分値によって，不必要な部分のみ0が乗算されることで焼失するので，全体をスパース化する効果ももつ  

2．重みの初期値設定  
各重みの初期値に乱数を適応し，ばらつきを持たせる．勾配消失によってどれかの重みが消失しても，他が生き残る可能性がある．  
さらに，単に正規分布に従った乱数による初期値を使うのではなく，前のノード数の平方根で除算した値を重みの初期値とするXavierという方法が提案されている．  
講義ではシグモイド関数への適応例も示されていたが，正規分布による乱数の場合に比べ，  
0－1の範囲で偏りがない結果になることが示されていた．  
LeRu関数などS字カーブではない関数に対してはXavierではなくHeという初期値設定手法が提案されている．  
Heでは重みの値を前の層のノード数の平方根で除算し，√２をかける．  

3．バッチ正則化  
ミニバッチ単位で入力値データの偏りを抑制する手法．  
活性化関数に値を渡す前後に、バッチ正規化を行う．  
これによって入力値を0－1で適切にばらつかせることが出来る．  
中間層の重みの更新が安定し，(収束までの)スピードもアップする．  
また、過学習を抑えることにもつながる．  
なお、コンピュータの特性を考慮し，ミニバッチサイズは2の倍数(または8の倍数)を用いることが多い．  


### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126127039-7af4d914-8792-464f-86bc-e5011efa9f0e.png)  
sigmoid - gauss  
![image](https://user-images.githubusercontent.com/87635559/126127196-37bb41c0-f733-425f-8bfc-8203c5c2111c.png)  

ReLU – gauss  
 
![image](https://user-images.githubusercontent.com/87635559/126127177-b06dc5c5-d11b-4633-8ed6-dd89e1dfc9ae.png)  

sigmoid - Xavier  
 
 
![image](https://user-images.githubusercontent.com/87635559/126127165-5926b331-08f8-4494-87fa-c37007eb4326.png)
ReLU - He  

シグモイド関数をReLu関数に変えるだけでも正解率は大幅に向上したが，Xavierを導入するとシグモイド関数も学習が進むようになり，勾配消失問題が解決されていることがわかる．  
ただし最も正解率が高いのはReLu+Heとなった．  
 
![image](https://user-images.githubusercontent.com/87635559/126127146-e4b91e8e-8757-43d2-89a6-bd485ed96c53.png)



### ・考察(および感想)
確認テストで重みの初期値に0を設定するとどのような問題が発生するかという話があった．  
すべての重みの値が均一に更新され，多数の重みをもつ意味がなくなってしまい、正しい学習が行えないと説明された．  
例えばある事柄を記憶する際，目視する，記述する，暗唱する，エピソードと関連付けるなど複数のアプローチをとった方が記憶に残りやすい．  
ニューラルネットは脳の仕組みを模しているといわれているが，重みは上記のような入力に対する重要度を表すので，各アプローチが等しく重要なのではなく，記憶に作用する重要なものとそうでないものあると思うと興味深い．  


## 学習率最適化手法
### ・講義のまとめ
勾配降下法における学習率をどのように調整するかは学習結果に影響する．  
学習率が大きすぎると発散し，小さすぎると収束までの時間がかかったり，極値にはまってしまう．  
学習率最適化の大きな指針としては初期の学習率を大きく設定し、徐々に小さくしていく．  
またはパラメータごとに学習率を可変させる．  
この学習率を最適化する手法がいくつか提案されている．  
講義では主に以下の4つが取り上げられた．  

１．モメンタム  
慣性を導入．局所最適解にならず，大域的最適解となる．  

２．AdaGrad  
(誤差関数が極端に深くなっていない)勾配の緩やかな斜面に対して最適値に近づける．しかし学習率が徐々に小さくなるので鞍点問題(大域的最適解に行きにくい)を引き起こす．  

3．RMSProp  
局所最適解にはならず、大域的最適解となる．とはいえAdaGradの改良なので似た動きをする．  

4．Adam  
モメンタムとRMSPropのメリットを併せ持つ(講師曰く最強の)最適化アルゴリズム．  



### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126128119-1ab2ab39-69b4-4d73-857e-1d510ccf8ba2.png)
SDG  

![image](https://user-images.githubusercontent.com/87635559/126128173-77b0318e-856d-4953-81d2-911b613c8e61.png)
モメンタム  

![image](https://user-images.githubusercontent.com/87635559/126128226-3796aaf4-1dec-4839-9e42-32c7def91f47.png)
AdaGrad  

![image](https://user-images.githubusercontent.com/87635559/126128269-8acab86c-6d68-4e06-a86e-78c900cb3a07.png)
RMSProp  

![image](https://user-images.githubusercontent.com/87635559/126128335-515fee9c-6e54-4972-b797-f6d8c9fb8d90.png)
Adam  

AdamとRMSprop以外は局所解に陥ってしまったようで全く正解率が向上しなかった．  

### ・考察(および感想)
講義のデモではいろんな学習例において各最適化がどのように動くかが示されていたが，必ずしも理論通りに最適化が機能するわけではなく，学習例によって各手法の向き不向きがあるようだ．  
Adamが最も優れていそうな印象は受けたが，もしAdamでうまく学習できない場合はSDGやモメンタムなどより基本的な手法を試す検討も必要だろう．  
機械学習、深層学習は手法がたくさんあるが，と期待問題に応じてそれらをどのように組み合わせるか探る必要がありそうだ．  

### ・追加調査

最新の最適化手法としてAdamの改良型であるAdaBeliefというものが提案されている．  
記事では多くの場合Adamと結果は変わらないが，Adamよりも優れた結果を示す事例も提示されていたので，何かデメリットがなければ今後のスタンダードになりそうな印象を受けた．  

“AdamはSGDに比べ収束が速かったり安定性が高いため、とても使いやすいオプティマイザーとして人気であり，デファクトスタンダードとしてよく用いられる．  
しかし必ずしもAdamがSGDよりも優れているわけではなくむしろ最終的な性能(汎化性能)はSGDのほうがAdamよりも良いことも多い．  
AdaBeliefはSGDのような「高い汎化性能」とAdamのような「速い収束性」「より良い安定性」の3つを兼ね備えたオプティマイザーとして大きな期待が寄せられている．“  

引用元  
https://qiita.com/omiita/items/5012afa3cba4d73a7aed  


その他、わかりやすかった最適化手法のまとめ記事  
https://qiita.com/omiita/items/1735c1d048fe5f611f80  


## 過学習
### ・講義のまとめ
特定の訓練サンプルに対して特化して学習している状態．  
特定のデータにはとてもうまくいくが，未知のデータに対してうまくいかない．  

ニューラルネットワークの大きさ，複雑さに対して入力値のデータ数が少ないと過学習が起こりやすい．ネットワークの自由度が高すぎると起こりやすい．   

抑制方法  
ネットワークの自由度(層数，ノード数、パラメータの値etc.)を制約すること．これを正則化という．  

　・L1, L2正則化  
　入ってきた値を過大評価している場所で過学習が起こる．極端な重みに調整されないように正則化項を付加する．  
　2点x,yに関して，ｘ＋ｙをp1ノルム(マンハッタン距離)，√(ｘ＋ｙ)をｐ２ノルム（ユークリッド距離）という．L1正則化ではp1ノルム、L2正則化ではp2ノルムを用いて計算を行う．またL1正則化，L2正則化はそれぞれラッソ回帰、リッジ回帰と呼ばれる．  

　・ドロップアウト  
　ノードの数の多いために過学習が起こることを踏まえた，ランダムにノードを削除して学習させる手法．データの連携にヴァリエーションが生まれるので，データ量を変化させずに異なるモデルを学習していることに等しい．  

### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126128950-9775834c-f42b-4acd-9345-817f8d60be48.png)  
過学習の状態  

![image](https://user-images.githubusercontent.com/87635559/126129168-75e30373-6125-465e-92c7-63df36ba82b0.png)  
L2正則化  

![image](https://user-images.githubusercontent.com/87635559/126129120-74e83fa0-cb99-4957-bf27-087263c45e7e.png)  
L1正則化  

![image](https://user-images.githubusercontent.com/87635559/126129079-0e1a62b3-4697-46a4-ac91-be28bef49fb2.png)  
ドロップアウト＋L1  



### ・考察(および感想)
ランダムに途中計算を放棄するドロップアウトを導入すると，よい結果をもたらすというのは面白い話だ．  
現実問題でも自分は進学先を考えるときにいくつかの志望校に関して最初は案内や評判を聞いて情報を集めたが，あれもこれもと調べて情報過多になったので判断する際には多くの情報を遮断し，優先するべきことのみ残した．  
そう考えるとドロップアウトは理にかなっている気もするが，このようなヒューリスティックなアプローチが取り入れられている時点で脳（の模倣）というのはまだ未解明の分野なのだと思った．  


### ・追加調査
過学習の回避方法  
以前の講義で登場したK-分割交差検証も過学習抑制の１つであるようだ．  
https://qiita.com/yut-nagase/items/d9fa748515bf14661cb2  


## 畳み込みニューラルネットワーク
### ・講義のまとめ
畳み込みニューラルネットワーク(CNN）では次元間で繋がりのあるデータを扱える  

CNNでは  
１．	次元のつながりを保ちつつ、特徴を抽出すること  
２．	その後，人間が欲しい結果に変換していくこと  
の2つが処理の大枠となる．  

CNNの肝は畳み込み処理にあるといえる．入力データにフィルターを適応し，中間データを作成する(特徴を抽出する、特徴量)　どのようなフィルタを適応すると欲しい特徴が抽出できるかよく考えることが大事だろう．  

CNNの例  
・LeNet  
入力層　32x32（=1024）  
中間層１　28,28,6（= 4704） 畳み込み  
中間層2　14,14,6（= 1176）　畳み込み  
中間層3　10,10,16（= 1600）　畳み込み  
中間層4　5,5,16（= 400）　畳み込み  
中間層5　120　全結合  
中間層6　84　全結合  
出力層　10  

畳み込み演算のための工夫  
・バイアス　…　過去の講義で登場したものと同じ 
・パディング　…　入力画像に対してフィルタを適応すると必ず出力画像が小さくなってしまう．これを回避するために入力画像の外側に１ピクセルを疑似的に拡張，0や隣接した数値で埋めることが多い  
・ストライド　…　フィルタの移動量．ストライドのサイズを大きくすると画像を小さくすることにつながる  
・チャンネル　…　フィルタ(全結合でいう重み)の数  

畳み込み演算が必要な理由  
全結合層のデメリットを解決するため  
→　画像の場合、縦、横、チャンネルの３次元データだが、1次元データとして処理されるため，それらの関連性が学習に反映されない．  

プーリング層  
対象領域の最大値、又は平均値を取得する処理(Max Pooling, Average Pooling)  

畳み込み処理後に出力される画像サイズの計算方法  
Oh = (height + 2 x padding – filter size) / stride + 1  
Ow = (width + 2 x padding – filter size) / stride + 1  



### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126129712-cac2618e-9c09-4551-a11b-11deb1c818b0.png)  

![image](https://user-images.githubusercontent.com/87635559/126129761-c689dfbc-8b18-4a8d-a937-9f06e3f94b1f.png)  


### ・考察(および感想)
CNNは単に画像を対象にした処理ではなく，全結合の問題点を解決するために畳み込み演算を行う必要があることはよく分かった．  

LeNetを見て思ったのは，なぜこういった中間層の構成にするとよい結果が得られるのか、例えばLeNetの中間層1は(30,30,6)や(28,28,10)ではだめなのか．  
なぜ中間層3は(10,10,12)ではなく(10,10,16)なのか．  
これら各層の数字やネットワーク構成には根拠があるのか、それとも色々試したらたまたま良い結果が得られただけなのか気になる．  
誰かが発表したネットワークを使うことは出来そうだが，自分でネットワークを考え出すのはハードルが高そうだ．  


## 最新のCNN
### ・講義のまとめ
AlexNet  
入力　224, 224  
中間層1  55,55,96  
中間層2  27,27,256  
中間層3  13,13,384  
中間層4  13,13,384  
中間層5  13,13,256  
中間層６　4096  
中間層7　4096  
出力　 　1000  

Flatten … 各層の数値を一直線に並べる処理  
Global Max Pooling … 各層の各チャンネルの最大値を抽出  
Global Average Pooling  … 各層の各チャンネルの平均値を抽出  

Global Max Pooling, Global Average Poolingともに大雑把な処理のわりにFlattenよりも良い結果が得られる．  

また、過学習を防ぐためにサイズ4096の全結合層の出力にドロップアウトを使用している．  


### ・実行結果のキャプチャ
(AlexNetとは異なるかもしれないが，より深いCNNの例として）2_8_deep_convolution_net.ipnbを実行してみた．  
確かに2_6_simple_convolution_net.ipnbよりも高い正答率が得られたが，処理時間はかなり増加した．  
(google colab GPUで約1.5時間)  

![image](https://user-images.githubusercontent.com/87635559/126130285-f1d1bc17-de45-47a8-9c4a-a8a58047f0b5.png)  


### ・考察(および感想)
Global Max Pooling, Global Average Poolingの話は興味深い．  
木を見て森を見ずではないが、人間が何かを認識するときもそこまで細かい部分まで見ていないということだろう．  


### ・追加調査
AlexNet以外にも様々なCNNの提案が行われているようなので，キーワードを中心に幾つかピックアップした．いずれも重要そうなモデルなので試験までに要追加調査．  
(ResNet, MobileNetはDay4の講義内で説明があったが，この記事はDay2終了時点に記述したもの)   

・VGGNet  
2014年提案．シンプルなモデルアーキテクチャや学習済みモデルが配布されたことから，現在においてもベースラインのモデルとして，またクラス分類以外のタスクのベースネットワークや特徴抽出器としても利用されている．  
本モデルを提案した論文の主な関心はCNNの深さがどのように性能に影響するかを明らかにすることであった．  
この目的のために，下記のようなモデルアーキテクチャの設計方針を明確にし，深さのみの影響が検証できるようにしている．  

・3×3（一部1×1）の畳み込みのみを利用する  
・同一出力チャネル数の畳み込み層を幾つか重ねた後にmax poolingにより特徴マップを半分に縮小する  
・max poolingの後の畳み込み層の出力チャネル数を2倍に増加させる  

上記の統一的な設計方針の元，ネットワークの深さを増加させていくとコンスタントに精度が改善することを示した．  


・ResNet  
2015年に提案された1つの大きなブレークスルーとなった手法．  
VGGNetで示されたように，ネットワークを深くすることは表現能力を向上させ，認識精度を改善するが，あまりにも深いネットワークは効率的な学習が困難であった．  
ResNetは，通常のネットワークのように，何かしらの処理ブロックによる変換F(x)F(x)を単純に次の層に渡していくのではなく，  
その処理ブロックへの入力xをショートカットし，H(x)=F(x)+xを次の層に渡すことが行われる．  
このショートカットを含めた処理単位をresidualモジュールと呼ぶ．  
ResNetでは，ショートカットを通して，backpropagation時に勾配が直接下層に伝わっていくことになり，  
非常に深いネットワークにおいても効率的に学習ができるようになった．  


・MobileNet  
組み込みデバイスや，スマートフォン等の，計算資源が潤沢ではない環境においては，高速に動作するモデルが重要となる．  
そこで高速化を意識したアーキテクチャとしてMobileNetが提案された．  
Depthwise Separable Convolutionという、畳み込みの計算を分けて行うことで、計算量が減らすことが肝となっている．  
通常、画像に対して畳み込みを行う場合、縦・横・奥行き（カラーチャンネルなど）といった多次元のデータに対して同時に畳み込みを行うが，  
それらを分割して計算しつつ、通常の畳み込みの計算と同じような結果を得る手法である．  

引用元  
https://qiita.com/yu4u/items/7e93c454c9410c4b5427  
https://qiita.com/simonritchie/items/f6d6196b1b0c41ca163c  

