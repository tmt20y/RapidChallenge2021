# 機械学習
##  線形回帰モデル
### ・講義のまとめ
シグマの計算は行列で表現可能  
手計算で行列計算を計算し，簡略化した表現への手計算での導出が出来ることが望ましい  
 (線形)回帰モデルは学習データの範囲外の予測、外挿は得意でない．実際に住宅価格予測の例題では，部屋数に１を指定するとマイナスの住宅価格が得られたが，どう考えても現実的には不適切である．  

### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126120089-59a25303-a751-4e19-87dd-9a58a1191725.png)  

### ・考察(および感想)
学習データの範囲外の予想が苦手というのはかなり致命的な印象．といっても未知のものに対しての予想結果の向上に関しては必ず手法や研究が存在していると思うので（ディープラーニング？），学習を進めながら注目しておきたい．  
また機械学習全般を通して数学的な理解が要求され，ステージテストでも知識というより計算を求められる問題が多かった．  
特にステージテストのロジスティック回帰の分類可否の問題(8,9)は資料中や検索では答えが見つからず，実際に自分でグラフを作成したり手を動かしてじっくり考えてみないと正解することが出来ず，かなり苦戦した．  
だが，現実のデータ分析の仕事では正解が与えられているわけではないと思うので，このように手を動かす，頭を使う問題を解く練習はE資格試験以外でも役に立つだろう．  

## 非線形回帰モデル
### ・講義のまとめ
線形回帰の数式でのxをφ(x)で置き換えたもの 
φ(x)を基底関数といい、高次多項式やガウス関数が用いられる  
非線形といって，φ(x)に関しては非線形であってもwに関しては線形である  
非線形回帰モデルでは高い表現力を持ったモデルを作成することが可能だが，表現力が高すぎると過学習に繋がってしまうので，不要な基底関数を削除して表現力を落とすことも大事．  
データから一定の割合で検証用に確保し，その他を学習用に確保，そして一度決めたデータ分割方法は変更しないで学習・検証を行う手法をホールドアウト法という．手元にデータが少ない場合は検証用データに外れ値が含まれやすいので要注意．  
これを回避するために検証、学習の組み合わせを変えて、複数回モデル学習・検証を行うクロスバリデーション法がある．  

### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126120202-6f196889-3552-4eef-9264-9cd33e648239.png)  
線形の場合  

![image](https://user-images.githubusercontent.com/87635559/126120244-0ddbfb90-89a9-487b-b364-7edc384b2044.png)  
非線形の場合  

### ・考察(および感想)
講義を通して、非線形回帰モデルは線形回帰モデルよりも表現力が高く、ほぼあらゆるデータを近似可能な印象も受けた．  
ただ，それと同時に学習用データに過剰適合させると，実際に予想したいデータに対しては良い結果が得られなくなることも分かった．  
一種のさじ加減のような印象も受けたが，実際の仕事ではデータを見つつどの程度が最適かという判断も出来るようになる必要がありそうだ．  

## ロジスティック回帰モデル
### ・講義のまとめ
分類問題に使われる．ある入力に対しての出力結果は０か１になる．シグモイド関数をもちいることで数値を０－１の範囲に収める．  
2017年のGoogleのAIブログではdeeplearningよりもロジスティック回帰の方が優れているという紹介もあったくらいで、バカにできない．  
識別的アプローチと生成的アプローチが存在し、生成的アプローチではベイズの定理をかませる．  
桁落ちを防ぐために（尤度関数の）計算にはlogを用いる．  
微分のchain ruleは深層学習でも使うので慣れておくとよい．  

### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126121334-35d31cd6-2eb9-4165-a5bc-3ad5511faa96.png)

### ・考察(および感想)
機械学習ではlogを理解しておくことがとても重要そうな気がした．  

また損失関数の最適化手法には勾配降下法やそれを改良した確率的勾配降下法，モメンタムなどいくつか手法がある．(数値計算の授業で習ったニュートン法も最適化の一種だろう)  
機械学習以外でも応用が利きそうなので，以下の記事等も参考に理解を深めておきたい．  
https://qiita.com/omiita/items/1735c1d048fe5f611f80  
https://qiita.com/m-hayashi/items/dab9d2f61c46df0a3a0a  

## 主成分分析
### ・講義のまとめ
100次元，200次元などの高次元のデータを低次元に圧縮する手法．  
PCAの圧縮の仕方としては分散が最大になる、データの散らばりが残るような低次元化を行う．  
分散が大きいということはそれだけデータの特徴が残っているということである．

### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126121306-879cfbf2-e940-4991-b3e5-a8d9ca51a8a8.png)

### ・考察(および感想)
たくさん与えられたデータからどれが大事な特徴のみを残しつつ，人が見て理解しやすい，計算が行いやすいデータに変換するための技術という印象．  
講義では駆け足だったが，サンプルを実行してみると，計算された重要な特徴に基づいたデータの類似度がグラフで表示され，データ可視化の際に有効な手段であることがよく分かった．

## アルゴリズム
### ・講義のまとめ
- k近傍法  
クラスタリングのための機械学習手法．  
入力に対する最近傍のデータをk個とってきて，それらが最も多く所属するクラスに分類する．  
kの値を変更すると結果が変わる．  
近くにいるものと同じところに分類するので直感的にもわかりやすい．多数決で決めるようなもの．

- k-means法  
教師なし学習のクラスタリング手法．  
与えられた複数のデータをk個のグループに分類する．  
各クラスタの中心の初期値を与えて計算をスタートするが，クラスタの初期値が近いと上手くクラスタリングできないので注意が必要．  

### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126121109-62a6e909-1094-4f9f-a21a-07e6c28ec365.png)  
knn

![image](https://user-images.githubusercontent.com/87635559/126121085-fb6b7617-127a-4bbe-9c32-ed452e91d61f.png)  
k-means  

### ・考察(および感想)
この２つは知っていた．どちらも比較的単純なアルゴリズムではあるが，データ分析現場でも使えるくらい汎用性が高いってことだろう．  

## サポートベクターマシン
### ・講義のまとめ
（オリジナルの）SVMは２クラス分類問題に使われる．  
与えられたデータを1/-1（またはA/Bなど）の２値に分類する分類境界を求める．  
さらにサポートベクトル分類はハードマージンとソフトマージンという２つの計算手法に分かれる．  
ハードマージンでは与えられた訓練データを完璧に分類できる分類境界が存在することを仮定している．  
一方，ソフトマージンでは必ずしも分類することが出来ない訓練データにも適応できるように拡張している．  
２つの計算では分類境界から一定の範囲(マージン)に存在するデータをどのように扱うか，そのマージンをどのように定義するか異なる．  
また円環状に広がったデータに関する分類のように線形SV分類が適応できないケースに対しては非線形な分類を行うことも可能．  
具体的には入力データの特徴ベクトルをより高次元空間に拡張し，そこで線形分離，その後もとの次元に戻すという計算を行う．  
さらにカーネル関数を用いて高次元空間での計算を簡略化(近似？)することも可能で，これをカーネルトリックという．  

### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126120558-e46708ce-9adc-46a2-a8b2-149397e22530.png)  
ハードマージン  

![image](https://user-images.githubusercontent.com/87635559/126120906-ede68c1e-21f2-45ac-a0d3-2f23e8b9e2be.png)  
ソフトマージン  

![image](https://user-images.githubusercontent.com/87635559/126120994-30af5e04-b996-499f-b268-118cbb73a0e6.png)  
非線形  

### ・考察(および感想)
SVMは講義動画内では説明されていなかったが，上記では触れなかったラグランジュ未定乗数の計算や双対問題など数学的にかなり難しい印象．  
しかしカーネルトリックなども含め，覚えておけば機械学習以外でも応用できそうという印象も受けたので，別の参考資料等で理解を深めておきたい．  
(SVMだけの本もありそうなくらい深い分野かも)  

### ・追加調査
わかりやすい解説記事  
https://qiita.com/c60evaporator/items/8864f7c1384a3c6e9bd9


