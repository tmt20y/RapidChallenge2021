# 応用数学
## 第1章：線形代数
### 講義のまとめ(と感想)
前半ではスカラー、ベクトルなど基本的な定義の確認から行列および逆行列の説明まで． 
逆行列の導出は行列の隣に単位行列をならべて、掃き出し法という計算手法を用いて求めるのがおすすめ  
逆行列が存在しない場合もあるので注意(行列式で確認可能)  
行列式は正方行列の大きさみたいなイメージ  

後半では固有値,固有ベクトルの求め方について一通り解説．  
特異値分解は少し難易度が高めだが，講師の方が概念を理解しておけば自分で計算できる必要はないと言われていたので信じよう．  
講義でも紹介されていたが，画像に特異値分解を応用すると，出来るだけ特徴を残しつつデータ圧縮を行うことも出来るそうだ．
(小さな特異値成分を減らしていくとが高周波成分を除去することに相当する)  

![image](https://user-images.githubusercontent.com/87635559/126157763-938fc655-4a21-40f5-bf31-f4e393a1dbfa.png)
![image](https://user-images.githubusercontent.com/87635559/126157773-149df3c1-c488-4e4d-86d0-828e459c8f9e.png)
![image](https://user-images.githubusercontent.com/87635559/126157780-0943983b-a605-44f8-8ad1-dcd6e51779b7.png)
![image](https://user-images.githubusercontent.com/87635559/126157789-c272a28e-cf3d-467c-994f-a8034f404cd3.png)  

特異値分解だけでなく、データから特徴を抽出，加工するにあたっては行列を用いることが多い．  
そのためにも行列演算を抑えておく必要がある．  


## 第2章：確率・統計
### 講義のまとめ(と感想) 
統計の講義で大事そうだったのは集合の概念とベイズ確率に関して．  
確率の問題を考えるときには集合としてのデータがどのようになっているか，図を書いて和集合や共通部分などを考えることが大事とのことだった．  
ベイズ確率に関しては，条件付確率やベイズ則，推定などが言葉の語順や言い回しに振り回される感じがして厄介に思う．  
しかし，確率・統計は行列同様，機械学習でも頻繁に必要になる数学だと思うので，やはりE資格受験を通して学びなおしておきたい．  


## 第3章：情報理論
### 講義のまとめ(と感想)
自己情報量  -log(P(x))  
情報の珍しさ, 事象が起こりやすいほど小さな値を返し，滅多に起こらない珍しいことに対しては大きな値になる定義式が欲しかったのでlogを使った．  
対数の底が2のとき単位はビット，底がネイピア数(e)のとき単位はnatという．  　
(熱力学のエントロピーとは異なるがイメージ的には似ているかもとのこと)  

シャノンエントロピー  H(x) = -E logP(x) = -ΣP(x)logP(x)  
自己情報量の期待値，誤差関数にも使われる  

カルバック・ライブラー(KL)ダイバージェンス  
DKL（P||Q） = Ex-p[log(P(x)/Q(x))] = Ex-p[logP(x) - logQ(x)]  
(E(x) = ∫P(x)f(x))  
同じ事象・確率変数に対して異なる確率分布の違いを表す，  
Pが新しく、Qが古い分布と仮定すると，新たな分布から古い分を見た状態  

交差エントロピー  
KLダイバージェンスの一部分を取り出したもの  
Qについての自己情報量をPの分布で平均している  
H(P, Q) = H(P) + DKL(P||Q) = - Ex-plogQ(x) = -ΣP(x)logQ(x)  

試験頻出分野のようだが、個人的には直感的でない部分が多く，理解するのが難しい．  
エントロピー、シャノンエントロピーまでは分かるが，KLダイバージェンス，交差エントロピーが釈然としない．  
試験までにネットで調べてもっとわかりやすい解説がないか調べておこう．  

