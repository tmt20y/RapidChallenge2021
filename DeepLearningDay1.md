# 深層学習 Day1
## 深層学習の基本概要
入力層、中間層、出力層による三層からなるネットワークによって学習する仕組み．数字の集まりを入力にとって、出力として確率を返す  

重み、バイアスを最適化していく  

何かしらの入力値をとって、何かしらの出力値を計算する変換器  

## 入力層―中間層
### ・講義のまとめ
入力層では入力とその入力の重要度を表す重み、入力をオフセットさせるためのバイアスを与える．  
それらをまとめて総入力といい中間層への入力として渡す．  
中間層では総入力と活性化関数を用いて出力を計算する．  
なお上記は行列を用いて計算を行う．

### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126122713-2cfbfc62-b26d-48af-a0e1-da03305a34ca.png)

### ・考察(および感想)
確認テストの動物の事例を考えると、入力層では身長、体重、ひげの本数、毛の平均長、耳の大きさ、眉間/目鼻の距離比、足の長さが入力として与えられる．  
実際に自分が動物を識別する場合でも，動物の見た目から上記のような特徴を(自動的に)抽出し、自身の過去の経験等に基づいた各項目の重要度(重み)によって脳内で無意識に識別計算を行っているのだろう．  
ディープラーニングは人間の脳を模倣しようとしているという話を聞いたことがあったが，その理由が少しわかった気がした．

## 活性化関数
### ・講義のまとめ
活性化関数とはニューラルネットワークにおいて、次の層への出力の大きさを決める非線形の関数であり，入力値によって次の層への出力のon/offや強弱を定めるために用いられる．  
中間層用と出力層用で異なる活性化関数が使われるようだ．  
中間層では機械学習の章でも学んだシグモイド関数がよく使われていたが，勾配消失問題を起こすことが分かったため，それを回避するためにReLu関数がよく使われるようになった．  

### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126123218-6d99a71d-a6f9-4f64-ab93-267088544fdd.png)


### ・考察(および感想)
実際のプログラムではシグモイド関数、ReLu関数ともにnumpyを用いればわずか１行で表現可能．  
サンプルコードを見ても、(ライブラリを使えば、かつシンプルな)ニューラルネットワークのプログラム自体はかなり短く作成できるのだろう．  
ただ、これらの部品をどのように組み合わせるのかが難しいのだろうが．  

### ・追加調査
現在でも活性化関数に関した研究が行われており，近年提案されたものを調査した．  

Mish  
　特徴  
　・上限なし  
　Sigmoidなどは1で飽和してしまい、その時の傾きはゼロとなってしまうため学習が遅くなるが、Mishは飽和しないため学習速度の低下を避けられる  
　・下限あり  
　強い正則化をかけることができる  
　・負の値を持つ  
　ReLUと違い、負の値も残る  
　・∞階微分まで関数が連続  
　ReLUは微分したら連続ではなくなるため勾配を用いるOptimizerでは予想外の問題を引き起こしかねないが、Mishは∞階微分まで連続であるため大丈夫  
　・Mishによるアウトプットランドスケープが滑らか  
　損失関数も滑らかになり、最適化しやすくなる．  

　引用元  
　https://ai-scholar.tech/articles/treatise/mish-ai-374  

また画像認識に特化した活性化関数としてFReLuというものも提案されているようだ．  
参考  
https://qiita.com/omiita/items/bfbba775597624056987  


## 出力層
### ・講義のまとめ
最終的な各クラスの確率を出力する層．  
さらに出力した結果は誤差関数を用いて正解とどのくらい合っているかを評価する．  
数値が小さければ正解に近く，大きければ正解に遠い(自信がない)ことになる．  
分類問題では誤差関数にクロスエントロピー誤差、回帰問題では平均二乗誤差が使われる．  
出力層での活性化関数は中間層の活性化関数とは目的が異なる．  
中間層の活性化関数は計情報を抽出することが目的だが，出力層の活性化関数は中間層を経て計算されてきた結果を使いやすいように変換することが目的となる．  
中間層が出力してきた情報を比率を変えずに、全体を足すと１になるように変換することが大事である．  
代表的なものとしては恒等写像やシグモイド関数、ソフトマックス関数がある．誤差関数と同じく活性化関数も回帰、二値分類、多クラス分類など目的に応じて使うべき関数が決まっている． 


### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126123599-56566cb7-5640-4c43-b50f-7549fb4cb144.png)


### ・考察(および感想)
(確認テストの問題として)平均二乗和にて数値を二乗しているのは正負の値を考慮せずに扱える方が都合がよいためであり，二乗することで常に正の値の誤差になるようにしているという話があった．  
(機械学習や)ニューラルネットは物理等に基づいた解析計算を行うのではなく，(脳の)シミュレーションのための数値計算を行っているので，二乗和誤差のように如何に計算を簡略化するかの工夫が大事な印象を受けた．  


## 勾配降下法
### ・講義のまとめ
深層学習の目的は学習を通してネットワークにおける誤差E(w)を最小にする重みパラメータw(とバイアスb)を求めることである．  
勾配降下法を用いてパラメータを最適化する．  
前回の計算結果をどの程度使うかを定めている学習率εをどのように定めるかが非常に重要になる．  
学習率が大きすぎると一回の計算での変化が大きくなり、場合によっては発散(解に収束しない)してしまう．  
一方，学習率が小さすぎると一回の計算での変化が小さく、発散することは少なくなるが収束までに時間がかかることになる．  
学習率の決定、収束性向上のためのアルゴリズムとしてはMomentum, Adamなど多数の研究が発表されている．  
勾配降下法によって誤差関数の値をより小さくする方向に重みとバイアスを更新し，更新した値を用いて再度入力層から計算を行う．この１周回をエポックという．  
また派生手法として機械学習の章でも登場した確率的勾配降下法(SDG)とミニバッチ勾配降下法が存在する．一般的にはミニバッチ勾配降下法が使われることが多い．  

### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126123851-091697cd-686d-4e10-9ac8-6574da6f537d.png)


### ・考察(および感想)
数値計算で解を求めることを踏まえれば，学習率をどう決めるかが大事であるというのはよくわかるし，研究テーマとなり得ることも理解に易い．  

### ・追加調査
学習の進め方としてオンライン学習とバッチ学習、ミニバッチ学習が紹介されていた．  
各学習法にどのようなメリット、デメリットがあるか調査し少しまとめてみた．  

・バッチ学習  
　学習対象となるデータを全てまとめて一括で処理  
  
　メリット  
　全データを用いた損失関数の変化を考えるので、学習結果が安定しやすい．全データ数が少ないときに有効な学習手法といえる．  
  
　デメリット  
　データが追加されるたびに一から学習しなおすので非効率的  

・ミニバッチ学習  
　N個の訓練データのなかから一部、n個を取り出し、パラメータの更新をする手法．取り出した訓練データをミニバッチと呼ぶ．  
  
　メリット  
　バッチ学習に比べて効率的  
　（GPU、SIMDを用いた）並列処理に適している  
　学習の停滞に陥りにくい．学習の停滞とは局所解に陥ることでパラメータの更新が行われなくなる現象を指す．  

　デメリット  
　データ数が少ないため、パラメータの変化に対して損失関数が敏感に反応する．(オンライン学習と同じように？)  

・オンライン学習  
　学習データが入ってくるたびにその都度、新たに入ってきたデータのみを使って学習を行う  

　メリット  
　1回の学習あたりのコストが低い  
　学習データを全て蓄えておく必要がない(メモリ)  
　ユーザ行動の変化にすぐに対応できる  

　デメリット  
　外れ値などノイズの影響を受けやすい  
　バッチ学習であれば事前にデータを確認して対処していたなどの状況を拾ってしまう可  
　最新のデータの影響を受けやすい  

参考
https://ai-trend.jp/basic-study/neural-network/sgd/
https://dev.classmethod.jp/articles/online-batch-learning/

＊ミニバッチ学習法はgoogleが特許を保持している(？)ようなので扱いに注意  
https://patents.google.com/patent/JP2017097585A/ja



## 誤差逆伝搬法
### ・講義のまとめ
誤差勾配を用いた重みの更新方法として数値微分を用いる方法が考えられるが、中間層が増えれば増えるほど計算負荷が大きくなってしまう．  
そこで誤差逆伝搬法が考案された．  
誤差逆伝搬法では算出された誤差を出力層側あら順に微分し，前層へと伝搬させていく．  
(微分連鎖律を用いて）解析的に計算することで，後ろの層での誤差を再利用することが出来るので，最小限の計算で各パラメータでの微分値を求めることが出来る．  

### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126124626-5db2a9e3-e2c6-43b2-8f51-49469b5d156b.png)


### ・考察(および感想)
微分の連鎖律を用いて計算を工夫，簡略化すること高校数学の問題でも扱ったが，AIにも応用されるような考え方であることに感銘を受けた．  
解析計算、数値計算両方をうまく組み合わせて解くべき問題を簡略化していくという考え方はニューラルネットワークに限らず，他の問題にも応用できそうであり，改めて数学の重要性を感じた．  

### ・追加調査
以下のブログで誤差逆伝搬法を用いた速度改善の例が紹介されていたので取り上げる．  
https://qiita.com/m-hayashi/items/c5ace811333364d887f5  

著者はscikit-learnのdatasetsにある手書き文字データを用いて、シンプルなニューラルネットワークの実装を行い、これに誤差逆伝播法を適応してどのような改善がみられるか検証した．  
結果として，認識精度は0.44%ほど低下したが，スピードは1969倍になったそうだ．  
誤差逆伝搬がいかに優れた発明であることがよくわかる事例である．  

