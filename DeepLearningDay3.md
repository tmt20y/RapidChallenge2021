# 深層学習Day3
## 再帰型ニューラルネットワーク(RNN)
### ・講義のまとめ

Recurrent Neural Network (RNN)  
時系列データに対応可能なニューラルネットワーク  
(時系列データ … 時間的なつながりがあるデータ　ex.音声データ、株価データ、テキストデータ)  

ネットワーク図  
![image](https://user-images.githubusercontent.com/87635559/126132924-a5b493cb-057c-4286-b4e5-0df27d67c5ef.png)  

中間層の出力を次の中間層への入力としても渡すことが大きな特徴．  
中間層から中間層への重みWも学習していく必要あり．  

BPTT(Back Propagation Through Time)　RNNでの誤差逆伝搬法  

順伝搬の計算  
![image](https://user-images.githubusercontent.com/87635559/126133115-8f5d5266-72d1-4ce2-8dad-84e2bab4ad83.png)  

逆伝搬の計算(で使う微分)  
![image](https://user-images.githubusercontent.com/87635559/126133251-1e6eba0c-7541-45a6-b47b-7a4ce5de3b55.png)

パラメータの更新式  
![image](https://user-images.githubusercontent.com/87635559/126133311-4b87b708-1862-44e1-b8dc-bc6c80152850.png)  
出力計算の中間層には既に時間的な成分が考慮されているので，Win, Wとは異なりWoutの更新では時間的に遡る必要がない点に注意． 


### ・実行結果のキャプチャ
![image](https://user-images.githubusercontent.com/87635559/126133364-c60ca1f9-9fc8-4f62-b047-a1eb8cd1a25f.png)  

![image](https://user-images.githubusercontent.com/87635559/126133430-1517d179-5186-4e37-98bd-0773cf67a1e4.png)  
Xavierでウェイト初期化　グラフの振動が多くなった（活性化関数はシグモイドのままなので改善はしないだろう）  

![image](https://user-images.githubusercontent.com/87635559/126133470-55463153-a3d6-4f9d-a828-e592554349d5.png)  
Heでウェイト初期化　収束が速くなった（活性化関数はシグモイド）  

![image](https://user-images.githubusercontent.com/87635559/126133536-9df611bf-7072-4624-85f9-f050850efe9d.png)  
ReLu関数に変更すると収束しない(勾配爆発)  


### ・考察(および感想)
上記サンプルをそのまま実行したにおいて，Epoc 4000付近でLossのスパイクが発生している．  
Heに変更した場合もやはりスパイクが生じているが、これはなぜか．  
RNNはある程度学習が進んでも不安定になりうるということか？  

プログラムは再帰構造を使うので，計算時間とメモリには注意が必要  


### ・追加調査
再帰計算は計算時間の増大が問題になることが多く，RNNでもBPTTの再帰処理がボトルネックになるようだ．  
そこで調べてみたところ，truncated BPTTという手法があるようだ．  
truncated BPTTでは数十から百程度の適当な時間ステップ数で逆誤差伝搬を打ち切る、つまり切り取られたネットワーク単位で学習を行うことになる．  
単に計算量だけでなく，時系列でつながりがあるとはいえ，あまりにも古い内容の重要度は低いと考えれば腑に落ちる手法だ．  
参考  
https://www.takapy.work/entry/2019/01/09/080338#Truncated-BPTT  


## LSTM
### ・講義のまとめ
RNNの課題…時間を遡るほど勾配が消失しやすい．また勾配爆発という逆伝搬が増えるほどに勾配が指数関数的に増加してしまう問題も起こり得る．  
RNNの構造を変えて勾配消失、勾配爆発の解決を試みたものがLSTM  
学習と記憶を別機能として持たせている  

![image](https://user-images.githubusercontent.com/87635559/126133963-46608e98-225a-4af9-a99e-f1746f80c44f.png)  

　・CEC  
　Constant error curcel  
　CECは過去の中間、入力などの情報を記憶するだけの機能を持つ  
　入力データについて時間依存度に関係なく重みが一律  
　→ニューラルネットワークによる学習特性が無い  

　・入力ゲートと出力ゲート  
　CECに何を覚えさせるか、何を出力させるか制御する  
　それぞれCECへの覚えさせ方，出力させ方をW, Uで学習する  

　・忘却ゲート  
　CECの過去の情報から要らなくなったものを忘れさせる機能  

　・覗き穴結合  
　CECのもつ値も入力ゲート、忘却ゲート、出力ゲートでの判断材料として用いることが出来るように，重み行列を介して伝搬可能にした構造．  
　(実際にはあまり大きな改善はしなかったようだ)  


### ・考察(および感想)
確認テスト  
LSTMを用いてXXXを予測する場合において，文中の“とても”という言葉は予測には影響しないと考えられる．  
「映画おもしろかったね．ところで，とてもおなかがすいたから何かXXX」  
→これは忘却ゲートの作用によって判断される．  
ということは，忘却ゲートは(情報が)重要か否かを判断しているということになるので、LSTMのネットワーク中でもかなり重要な役割を果たしていると言えそうだ．  

RNNの中間層に関して明確な役割分担をさせたものという印象  
改善点もあるが、その分複雑な構成になっている．  
CNN，RNNに比べて実装するのは大変そうな印象．  

### ・追加調査
端的にかかれた資料  
https://speakerdeck.com/ogawayohei/lstmtozui-xin-falselun-wen-shao-jie?slide=2  

詳しく書かれた資料  
https://qiita.com/t_Signull/items/21b82be280b46f467d1b  

一部を抜粋すると著者は以下のようにLSTMを評価している  
・LSTMは時系列データに対する強力な予測モデルである  
・勾配消失問題を解決し、長期の時系列を学習可能になった  
・LSTMは強力なモデルだが、やみくもに動かしても意味のある出力を得ることは不可能に近い、緻密な問題設定が必要  

特に３つ目は自分も同じ印象を受けた．設計、プログラムともに複雑になりそうで正直あまり扱いたくはない．  

## GRU
### ・講義のまとめ
LSTMの問題、パラメータが多く計算時間がかかる  
GRUではパラメータを大幅に削減し、精度は同等又はそれ以上が望めるようにした  

![image](https://user-images.githubusercontent.com/87635559/126134496-6a5408d1-104d-4d60-bf18-627bfd6266e5.png)  

CEC，入力ゲート、出力ゲート、忘却ゲートを廃止  
リセットゲート、更新ゲートを導入  
隠れ層(h(t))に情報を保持 ← 重要  


### ・実行結果のキャプチャ
メモ：predict_word.ipynbの先頭に以下を追加  
%tensorflow_version 1.x  

![image](https://user-images.githubusercontent.com/87635559/126134645-a432c1f5-607c-4a48-851c-f5f3b7d8f862.png)  
結果がわかりやすいように表示を修正．  
“some of them looks like” の後には“et”が最適との答えが得られた．？な結果となった．  

![image](https://user-images.githubusercontent.com/87635559/126134709-1b9055d6-a900-42f5-a342-7ff799ac850f.png)  
別の文章を入れて予測を行ってみたが、全くそれらしい結果は得られなかった…  



### ・考察(および感想)
ネットワークがシンプルになったので計算効率が改善したことは想像できるが，  
実際LSTMと比較するとどの程度高速になったのか，例えば演習で行ったpredict_wordと全く同じ処理をLSTMで行った場合，どの程度時間がかかるのかが気になる．  

また，演習ファイルがおそらく意図した動作をしていない気がするので、ネットで別の開発例などを探して試してみたい．  

### ・追加調査

比較事例がないか検索してみたら以下の記事が目に留まった．  
https://qiita.com/MeiContact/items/65959ad543dfe39f3e99  

ノイズを加えた波のような学習データを入力していたが，結果としては速度はRNN、LSTM、GRUの順になった模様．どんな問題に対してもGRUが拘束というわけではないのだろう．  
残念ながらシンプルすぎる事例でGRUの良さが感じられない結果だった．  
E資格を受験された方が書かれた内容で，RNN、LSTM、GRUがよくまとまっているので今後の勉強の参考にしたい．  


## 双方向RNN
### ・講義のまとめ
過去の情報だけでなく、未来の情報を加味することで精度を高めるためのモデル．  
下図のように順方向（過去から未来への中間層のつながり）と、逆方向(未来から過去への中間層のつながり)、双方向に情報を受け渡していく．  
実用例として文章の推敲や機械翻訳などがある．  
![image](https://user-images.githubusercontent.com/87635559/126135222-2279afaa-f46b-4353-896e-1335631d5959.png)  


### ・考察(および感想)

演習チャレンジで以下のコードにおける* 行の穴埋めを行った  

def bidirectional_rnn_net(xs, W_f, U_f, W_b, U_b, V):  
　xs_f = np.zeros_like(xs)  
　xs_b = np.zeros_like(xs)  
　for i, x in enumerate(xs):  
　　xs_f[i] = x  
　　xs_b[i] = x[::-1]  
　hs_f = _rnn(xs_f, W_f, U_f)  
　hs_b = _rnn(xs_b, W_b, U_b)  
　* hs = [np.concatenate([h_f, h_b], axis=1) for h_f, h_b in zip(hs_f, hs_b)]  
　ys = hs.dot(V.T)  
　return ys  

実際の問題を想定していると思うので，双方向RNNのアルゴリズムとnumpyの関数(おそらく基本的なものだとは思うが)を両方押さえておかなければならなそうだ…．  

## Seq2Seq
### ・講義のまとめ
機械翻訳に用いられている手法．以下の図のように２つのニューラルネットワークをドッキングさせている．  
１つ目のネットワークに単語を入力，隠れ層には単語の意味がベクトル表現として保持されていく．保持した文脈情報を２つ目のネットワークに入力し別の表現に作り替える．英語で文を聞いて日本語で説明しなおす翻訳のようなイメージ．  
１つ目のネットワークをEncoder, ２つ目をDecoderという．  
(単語など)データの並びから別の並びを作り出すのでseq2seqという．  
![image](https://user-images.githubusercontent.com/87635559/126135896-82c18e56-cf33-41ff-aba5-c4946793b16c.png)  

自然言語処理の基礎(？)  
One-hot vector  
単語を0,1の並びで表現したベクトル．  
単語ごとにIDを振り、対応したベクトルを定める．  
例えば１００００語あれば１００００次元のベクトルで表現する．  

Embedding  
one-hotベクトルでは無駄に次元が大きくなるので，数百次元に収まるように0,1以外の数値も用いて表現するようにしたもの．  
機械学習を用いて計算する．  
近いベクトルは似た意味を持つ単語を示す．つまり単語の意味を抽出したベクトルと言える．  

Ex.  
単語　ID　one-hot        embedding  
私　　1   [1, 0, 0, …, 0]  [0.2, 0.4, 0.6, …]  
は    2   [0, 1, 0, …, 0]  [0.8, 0.7, 0.1, …]  

HRED  
“一問一答しかできない“というSeq2seqの課題に対する改良としてHREDというモデルがある．  
単語だけでなく文脈も考慮する．RNNを複数組み合わせている複雑そうなモデル．  

VHRED  
オートエンコーダの潜在変数の概念を追加したHREDの改良形．  

オートエンコーダ  
教師なし学習の仕組み．  
Encoder-Decoderからなる，入力を潜在変数ｚに変換し，ｚをインプットとして元データの復元を行うネットワークモデル.　  
画像でも使われる．  
メリットは次元削減が出来ること．  

VAE  
オートエンコーダの改良．潜在変数zに確率分布を導入したもの．  
近いデータは近いベクトルに、遠いデータは遠いベクトルになる．  
ノイズを加えながら学習することでより汎用的な能力の獲得を目指す．  


### ・考察(および感想)
確認テスト  
Seq2seqの説明  
→2. RNNを用いたEncoder-Decoderモデルの一種であり，機械翻訳などのモデルに使われる．  
実用を意識して作られたモデルで面白いと思った．翻訳など自然言語以外でも応用例があるのか気になる．  

Seq2seq，HRED、VHREDの違い  
Seq2seq 一文の一問一答に対して処理が出来るある時系列データからある時系列データを作り出すネットワーク  

HRED seq2seqの機構にそれまでの文脈の意味ベクトルを解釈に加えられるようにすることで，文脈の意味をくみ取った文の変換を出来るようにしたもの  

VHRED HREDが文脈に対して当たり障りのない回答しか作れなくなったことに対しての解決策．オートエンコーダ，VAEの考え方を取り入れて短い，当たり障りのない単語以上の出力を出せるように改良を施したモデル  

### ・追加調査
オートエンコーダは汎用性が高そうなので追加調査した．  

主成分分析との比較  
同じ次元削減の手法であるPCAとの違いが気になったので調べたが，明確な優劣、向き不向きがあるという内容は見つけられなかった．  

以下によると(数学的にみて)PCAはAEの特殊な場合であり，PCA∈AEとのこと．  
https://jp.quora.com/%E8%87%AA%E5%8B%95%E7%AC%A6%E5%8F%B7%E5%8C%96%E5%99%A8-%E3%82%AA%E3%83%BC%E3%83%88%E3%82%A8%E3%83%B3%E3%82%B3%E3%83%BC%E3%83%80-%E3%81%A8%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-PCA-%E3%81%AE%E5%87%BA%E5%8A%9B  

向き不向きなデータがあると思うのだが，  
ニューラルネットワークに向いている例(つまりデータを大量に用意して力業でなんとかしたい場合)であればAEを使えばいいってこと？  

ノイズ除去や異常検知への応用  
VAEではノイズを加えて復元の汎用性を持たせるという話があったが，逆に画像や音声からのノイズ除去に応用したり，異常検知に使われるようだ．  
https://products.sint.co.jp/aisia/blog/vol1-21  
https://qiita.com/cvusk/items/019c254db883957b3050  

非常に優れた手法かと思いきや、以下のような記事も見つけた．  

“では、オートエンコーダが今のディープラーニングを支えているのかというと、そうでもなさそうだ。深層学習ライブラリKerasのオートエンコーダのチュートリアルには、もう今では実用的な用途としてはめったに使われてないと書かれている。オートエンコーダは画像のノイズ除去や可視化程度でしか利用目的がないとされている。”  

引用元  
https://deepage.net/deep_learning/2016/10/09/deeplearning_autoencoder.html  

調べた範囲のことをまとめておくと、  
・オートエンコーダは汎用性が高いが現在ではディープラーニングではあまり使われなくなった古典的な手法  
・しかしノイズ除去や異常検知など特定の目的に使われる  
・PCAと差別化できるかは不明  


## Word2Vec
### ・講義のまとめ
単語をベクトル表現する手法．Embedding表現を得る手段の１つ．  
大規模データの分散表現の学習が現実的な計算速度とメモリ量で可能になった．  
One-hotベクトルの単語を入力すると意味を保ちながら小さなベクトル表現に変換してくれる変換ベクトル(表)を機械学習によって構築する．  


### ・追加調査
参考サイト  
分かりやすい解説  
https://qiita.com/g-k/items/69afa87c73654af49d36  

word2vecを使うことで以下のように言葉が計算可能になるらしい  
King – Masculinity + Femininity = Queen（”王様” – “男” – “女” = “女王”）  
https://ainow.ai/2021/04/08/254071/  

また，顔文字をベクトル表現に変換し、足し算を行うEmoji2Vecというものを以下のサイトで見つけた．  
https://www.pc-koubou.jp/magazine/39998  

😠 + 😭 = 😫  
😭 + 😆 = 😂  
となるらしい．  

言葉や絵を意味を解釈しつつ，ベクトルに変換するというのは非常に面白い考え方だと思う．  


## Attention Mechanism
### ・講義のまとめ
Seq2seqの問題として長い文章に対応できない．  
隠れ層の大きさが決まっているため，２単語でも、１００単語でも固定次元ベクトルに中に入力しなければならず，長い文章の意味を表現することが出来ない．  

そこで一文の中で重要な単語を見つける仕組みとしてAttention Mechanismが提案された．  
中間層の情報量が一定であっても，重要な単語を漏らさずに扱うことが出来る．  
非常に強力で近年の自然言語処理は全部Attention Mechanismを使っているらしい．  

### ・考察(および感想)
RNN 時系列データを処理するのに適したニューラルネットワーク  

Word2vec 単語の分散表現ベクトルを得る手法  

Seq2seq １つの時系列データから別の時系列データを得るネットワーク  

Attention 時系列データの中身に関して関連性に重みをつける手法  

自然言語処理でキーとなる一連の深層学習手法を知ることが出来た．  
ここ2，3年でgoogle翻訳やdeeplなど機械翻訳サービスが急速に発展したが，その裏は上記のような発明の連続で成り立っているということがよく分かった．  


### ・追加調査

こちらの資料がRNN全般に関して非常によくまとまっていた  
https://stanford.edu/~shervine/l/ja/teaching/cs-230/cheatsheet-recurrent-neural-networks  

Attention Mechanismは画像との紐づけにも応用できるようだ．  

![image](https://user-images.githubusercontent.com/87635559/126132838-ec3ff07b-ed3e-46f0-a297-e6fd999ded5a.png)

画像は上記サイトから引用  


